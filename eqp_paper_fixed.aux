\relax 
\providecommand\hyper@newdestlabel[2]{}
\providecommand\HyperFirstAtBeginDocument{\AtBeginDocument}
\HyperFirstAtBeginDocument{\ifx\hyper@anchor\@undefined
\global\let\oldcontentsline\contentsline
\gdef\contentsline#1#2#3#4{\oldcontentsline{#1}{#2}{#3}}
\global\let\oldnewlabel\newlabel
\gdef\newlabel#1#2{\newlabelxx{#1}#2}
\gdef\newlabelxx#1#2#3#4#5#6{\oldnewlabel{#1}{{#2}{#3}}}
\AtEndDocument{\ifx\hyper@anchor\@undefined
\let\contentsline\oldcontentsline
\let\newlabel\oldnewlabel
\fi}
\fi}
\global\let\hyper@last\relax 
\gdef\HyperFirstAtBeginDocument#1{#1}
\providecommand\HyField@AuxAddToFields[1]{}
\providecommand\HyField@AuxAddToCoFields[2]{}
\citation{scellier17}
\citation{hopfield1984}
\citation{bengio2015}
\citation{scellier17}
\citation{scellier17}
\citation{mnist1998}
\citation{simonyan2014,srivastava2015tvdn}
\citation{scellier17}
\citation{watts98}
\citation{bartunov2018}
\citation{scellier17}
\citation{bullmore2009}
\citation{he2015,srivastava2015}
\citation{xiaohu2011,krishnan2019}
\providecommand \oddpage@label [2]{}
\@writefile{toc}{\contentsline {section}{Abstract}{1}{section*.1}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {1}Introduction}{1}{section.1}\protected@file@percent }
\citation{scellier17}
\citation{scellier17}
\citation{hopfield1984}
\citation{bengio2015}
\citation{scellier17}
\citation{scellier17}
\citation{scellier17}
\citation{scellier17}
\citation{scellier17}
\@writefile{toc}{\contentsline {section}{\numberline {2}Background and Theory}{2}{section.2}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {2.1}Equilibrium propagation}{2}{subsection.2.1}\protected@file@percent }
\newlabel{sec:eqp_formulation}{{2.1}{2}{Equilibrium propagation}{subsection.2.1}{}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {2.1.1}Implementation in a continuous Hopfield network}{2}{subsubsection.2.1.1}\protected@file@percent }
\newlabel{eqn:hardened_sigmoid}{{1}{2}{Implementation in a continuous Hopfield network}{equation.2.1}{}}
\newlabel{eqn:cost}{{4}{2}{Implementation in a continuous Hopfield network}{equation.2.4}{}}
\newlabel{eqn:dynamics}{{6}{2}{Implementation in a continuous Hopfield network}{equation.2.6}{}}
\newlabel{eqn:weight_correction}{{7}{2}{Implementation in a continuous Hopfield network}{equation.2.7}{}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {2.1.2}Approximation of equation of motion}{2}{subsubsection.2.1.2}\protected@file@percent }
\citation{lillicrap2014}
\citation{oconnor2018}
\citation{bullmore2009}
\citation{scellier17}
\citation{glorot2010}
\citation{schmidhuber2015}
\citation{scellier17}
\citation{watts98}
\citation{bullmore2009}
\citation{watts98}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {2.1.3}Related methods that could enhance biological-plausibility}{3}{subsubsection.2.1.3}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {2.2}Vanishing gradient problem}{3}{subsection.2.2}\protected@file@percent }
\newlabel{sec:vangrad}{{2.2}{3}{Vanishing gradient problem}{subsection.2.2}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.3}Small-world networks}{3}{subsection.2.3}\protected@file@percent }
\newlabel{sec:sw_background}{{2.3}{3}{Small-world networks}{subsection.2.3}{}}
\newlabel{sec:sw_algorithm}{{2.3.1}{3}{Algorithm for generating a small-world network}{subsubsection.2.3.1}{}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {2.3.1}Algorithm for generating a small-world network}{3}{subsubsection.2.3.1}\protected@file@percent }
\citation{scellier17}
\citation{scellier17}
\citation{scellier17}
\citation{pytorch}
\citation{scellier17}
\citation{mnist1998}
\citation{scellier17}
\citation{scellier17}
\citation{glorot2010}
\citation{scellier17}
\providecommand*\caption@xref[2]{\@setref\relax\@undefined{#1}}
\newlabel{fig:top_basic}{{1a}{4}{Topology of the basic multilayer network tested in \cite {scellier17}. All pairs of neurons in adjacent layers are connected, and there are no additional connections. The learning rate for weights is reduced by a factor of 4 each time distance from the output decreases by one layer, to compensate for the vanishing gradient problem (section \ref {sec:vangrad}).\relax }{figure.caption.5}{}}
\newlabel{sub@fig:top_basic}{{a}{4}{Topology of the basic multilayer network tested in \cite {scellier17}. All pairs of neurons in adjacent layers are connected, and there are no additional connections. The learning rate for weights is reduced by a factor of 4 each time distance from the output decreases by one layer, to compensate for the vanishing gradient problem (section \ref {sec:vangrad}).\relax }{figure.caption.5}{}}
\newlabel{fig:top_sw}{{1b}{4}{Changes we have made to the basic topology to compensate for the vanishing gradient problem (section \ref {sec:vangrad}) while using a single learning rate for all weights. Red dotted lines denote connections that have been removed and blue lines denote their replacements. Green lines denote added connections within layers (these are also candidates for replacement). In this illustration layers have been made fully connected, and each connection has then been replaced by a random layer-skipping connection with probability $p\approx 8\%$.\relax }{figure.caption.5}{}}
\newlabel{sub@fig:top_sw}{{b}{4}{Changes we have made to the basic topology to compensate for the vanishing gradient problem (section \ref {sec:vangrad}) while using a single learning rate for all weights. Red dotted lines denote connections that have been removed and blue lines denote their replacements. Green lines denote added connections within layers (these are also candidates for replacement). In this illustration layers have been made fully connected, and each connection has then been replaced by a random layer-skipping connection with probability $p\approx 8\%$.\relax }{figure.caption.5}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {1}{\ignorespaces \relax }}{4}{figure.caption.5}\protected@file@percent }
\newlabel{fig:topology_illus}{{1}{4}{\relax }{figure.caption.5}{}}
\@writefile{toc}{\contentsline {section}{\numberline {3}Implementation}{4}{section.3}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {3.1}Basic topology with unique learning rates}{4}{subsection.3.1}\protected@file@percent }
\newlabel{sec:basic_topology}{{3.1}{4}{Basic topology with unique learning rates}{subsection.3.1}{}}
\newlabel{eqn:gb_init}{{11}{4}{Basic topology with unique learning rates}{equation.3.11}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.2}Basic topology with single learning rate}{4}{subsection.3.2}\protected@file@percent }
\newlabel{sec:basic_topology_uniform}{{3.2}{4}{Basic topology with single learning rate}{subsection.3.2}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.3}Our topology}{4}{subsection.3.3}\protected@file@percent }
\newlabel{sec:our_topology}{{3.3}{4}{Our topology}{subsection.3.3}{}}
\citation{scellier17}
\citation{scellier17}
\citation{mnist1998}
\@writefile{lot}{\contentsline {table}{\numberline {1}{\ignorespaces Hyperparameters of networks tested on MNIST dataset\relax }}{5}{table.caption.7}\protected@file@percent }
\newlabel{table:hyperparameters}{{1}{5}{Hyperparameters of networks tested on MNIST dataset\relax }{table.caption.7}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.4}Tracking the training rates of individual pairs of layers}{5}{subsection.3.4}\protected@file@percent }
\newlabel{eqn:rms_correction}{{12}{5}{Tracking the training rates of individual pairs of layers}{equation.3.12}{}}
\newlabel{eqn:running_avg}{{13}{5}{Tracking the training rates of individual pairs of layers}{equation.3.13}{}}
\@writefile{toc}{\contentsline {section}{\numberline {4}Results}{5}{section.4}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {2}{\ignorespaces Comparison of performance of network topologies on MNIST dataset. Dotted lines show test error and solid lines show training error. In red is a network with the basic topology and a single learning rate (section \ref  {sec:basic_topology_uniform}). In blue is a network with the basic topology and unique learning rates (section \ref  {sec:basic_topology}), tuned to counter the vanishing gradient problem; this is a recreation of the 5-layer network in \citep  {scellier17}. In green is a network with our topology, $p=7.56\%$ (section \ref  {sec:our_topology}). The network with a basic topology and a single learning rate performs poorly because it suffers from the vanishing gradient problem. The problem can be solved by introducing unique learning rates, or by implementing our topology.\relax }}{5}{figure.caption.6}\protected@file@percent }
\newlabel{fig:mnist_comparison}{{2}{5}{Comparison of performance of network topologies on MNIST dataset. Dotted lines show test error and solid lines show training error. In red is a network with the basic topology and a single learning rate (section \ref {sec:basic_topology_uniform}). In blue is a network with the basic topology and unique learning rates (section \ref {sec:basic_topology}), tuned to counter the vanishing gradient problem; this is a recreation of the 5-layer network in \cite {scellier17}. In green is a network with our topology, $p=7.56\%$ (section \ref {sec:our_topology}). The network with a basic topology and a single learning rate performs poorly because it suffers from the vanishing gradient problem. The problem can be solved by introducing unique learning rates, or by implementing our topology.\relax }{figure.caption.6}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.1}Network performance comparison}{5}{subsection.4.1}\protected@file@percent }
\newlabel{sec:network_performance}{{4.1}{5}{Network performance comparison}{subsection.4.1}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.2}Training rates of individual pairs of layers}{5}{subsection.4.2}\protected@file@percent }
\newlabel{sec:mnist_perlayer}{{4.2}{5}{Training rates of individual pairs of layers}{subsection.4.2}{}}
\citation{he2015}
\@writefile{lof}{\contentsline {figure}{\numberline {3}{\ignorespaces Extent of the vanishing gradient problem in different network topologies while training on MNIST dataset. To the left is a network with the basic topology and a single learning rate (section \ref  {sec:basic_topology_uniform}), in the center is a network with the basic topology and unique learning rates (\ref  {sec:basic_topology}), and to the right is a network with our topology, $7.56\%$ (section \ref  {sec:our_topology}). In the basic topology with a single learning rate, the training rate of a layer decreases roughly by a factor of 4 each time its distance from the output increases by 1 layer. In the basic topology with unique learning rates, this effect is countered by increasing the learning rate of a layer by a factor of 4 each time its distance from the output increases by one layer; as a result, the training rates are very uniform. In the network with our topology, the effect is countered, albeit less-effectively, by replacing a subset of connections by random layer-skipping connections.\relax }}{6}{figure.caption.8}\protected@file@percent }
\newlabel{fig:mnist_layers}{{3}{6}{Extent of the vanishing gradient problem in different network topologies while training on MNIST dataset. To the left is a network with the basic topology and a single learning rate (section \ref {sec:basic_topology_uniform}), in the center is a network with the basic topology and unique learning rates (\ref {sec:basic_topology}), and to the right is a network with our topology, $7.56\%$ (section \ref {sec:our_topology}). In the basic topology with a single learning rate, the training rate of a layer decreases roughly by a factor of 4 each time its distance from the output increases by 1 layer. In the basic topology with unique learning rates, this effect is countered by increasing the learning rate of a layer by a factor of 4 each time its distance from the output increases by one layer; as a result, the training rates are very uniform. In the network with our topology, the effect is countered, albeit less-effectively, by replacing a subset of connections by random layer-skipping connections.\relax }{figure.caption.8}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.3}Error rate after one epoch as connections are added}{6}{subsection.4.3}\protected@file@percent }
\newlabel{sec:mnist_1epoch}{{4.3}{6}{Error rate after one epoch as connections are added}{subsection.4.3}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {4}{\ignorespaces Performance of a network with our topology (section \ref  {sec:our_topology}) with varying $p$. The top graph shows the training error after one epoch. The bottom graph shows the extent to which weights connecting each pair of layers was corrected over the epoch. It can be seen that there is little improvement for $p<10^{-4}$, rapid improvement for $10^{-4}<p<10^{-2}$ and little improvement for $p>10^{-2}$. The training error after one epoch decreases as pairs of layers train more uniformly with respect to depth.\relax }}{6}{figure.caption.9}\protected@file@percent }
\newlabel{fig:mnist_1epoch}{{4}{6}{Performance of a network with our topology (section \ref {sec:our_topology}) with varying $p$. The top graph shows the training error after one epoch. The bottom graph shows the extent to which weights connecting each pair of layers was corrected over the epoch. It can be seen that there is little improvement for $p<10^{-4}$, rapid improvement for $10^{-4}<p<10^{-2}$ and little improvement for $p>10^{-2}$. The training error after one epoch decreases as pairs of layers train more uniformly with respect to depth.\relax }{figure.caption.9}{}}
\@writefile{toc}{\contentsline {section}{\numberline {5}Discussion}{6}{section.5}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {5.1}Small-world metrics have little correlation with network performance}{6}{subsection.5.1}\protected@file@percent }
\citation{he2015,ioffe2015}
\citation{lee2015}
\citation{xie2003}
\citation{pineda1987}
\citation{lillicrap2014}
\citation{oconnor2018}
\citation{bengio2015}
\citation{???}
\citation{bartunov2018}
\citation{shainline2019}
\citation{davies2018}
\citation{nahmias2013}
\citation{he2015}
\citation{srivastava2015}
\citation{xiaohu2011}
\citation{krishnan2019}
\citation{ioffe2015}
\citation{glorot2010}
\citation{???}
\bibstyle{plain}
\bibdata{references}
\bibcite{bartunov2018}{{1}{}{{}}{{}}}
\bibcite{bengio2015}{{2}{}{{}}{{}}}
\bibcite{bullmore2009}{{3}{}{{}}{{}}}
\bibcite{davies2018}{{4}{}{{}}{{}}}
\bibcite{glorot2010}{{5}{}{{}}{{}}}
\bibcite{he2015}{{6}{}{{}}{{}}}
\bibcite{hopfield1984}{{7}{}{{}}{{}}}
\@writefile{toc}{\contentsline {subsection}{\numberline {5.2}Nonlinearities learning residuals}{7}{subsection.5.2}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {6}Related work}{7}{section.6}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {7}Conclusion}{7}{section.7}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{References}{7}{section*.11}\protected@file@percent }
\bibcite{humphries2008}{{8}{}{{}}{{}}}
\bibcite{ioffe2015}{{9}{}{{}}{{}}}
\bibcite{krishnan2019}{{10}{}{{}}{{}}}
\bibcite{mnist1998}{{11}{}{{}}{{}}}
\bibcite{lee2015}{{12}{}{{}}{{}}}
\bibcite{lillicrap2014}{{13}{}{{}}{{}}}
\bibcite{nahmias2013}{{14}{}{{}}{{}}}
\bibcite{oconnor2018}{{15}{}{{}}{{}}}
\bibcite{pineda1987}{{16}{}{{}}{{}}}
\bibcite{scellier17}{{17}{}{{}}{{}}}
\bibcite{schmidhuber2015}{{18}{}{{}}{{}}}
\bibcite{shainline2019}{{19}{}{{}}{{}}}
\bibcite{simonyan2014}{{20}{}{{}}{{}}}
\bibcite{srivastava2015tvdn}{{21}{}{{}}{{}}}
\bibcite{srivastava2015}{{22}{}{{}}{{}}}
\bibcite{watts98}{{23}{}{{}}{{}}}
\bibcite{xiaohu2011}{{24}{}{{}}{{}}}
\bibcite{xie2003}{{25}{}{{}}{{}}}
\citation{watts98}
\citation{humphries2008}
\newlabel{tocindent-1}{0pt}
\newlabel{tocindent0}{0pt}
\newlabel{tocindent1}{6.25499pt}
\newlabel{tocindent2}{11.49998pt}
\newlabel{tocindent3}{20.22pt}
\providecommand\NAT@force@numbers{}\NAT@force@numbers
\@writefile{toc}{\contentsline {section}{\numberline {A}Metrics of a graph's small-worldness}{8}{appendix.1.A}\protected@file@percent }
\newlabel{app:sw_metrics}{{A}{8}{Metrics of a graph's small-worldness}{appendix.1.A}{}}
\newlabel{eqn:charpathlength}{{14}{8}{Metrics of a graph's small-worldness}{equation.1.A.14}{}}
\newlabel{eqn:clustcoeff}{{15}{8}{Metrics of a graph's small-worldness}{equation.1.A.15}{}}
\@writefile{toc}{\contentsline {section}{\numberline {B}Influence of approximation of differential equation of motion on vanishing gradient problem}{8}{appendix.1.B}\protected@file@percent }
\newlabel{TotPages}{{8}{8}{}{page.8}{}}
