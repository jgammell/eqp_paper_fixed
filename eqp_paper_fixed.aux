\relax 
\citation{scellier17}
\citation{hopfield1984}
\citation{bengio2015}
\citation{mnist1998}
\citation{watts98}
\citation{bartunov2018}
\citation{bullmore2009}
\citation{he2015}
\citation{srivastava2015}
\citation{xiaohu2011}
\citation{krishnan2019}
\@writefile{toc}{\contentsline {section}{\numberline {1}Introduction}{1}\protected@file@percent }
\citation{scellier17}
\citation{hopfield1984}
\citation{bengio2015}
\citation{scellier17}
\@writefile{toc}{\contentsline {section}{\numberline {2}Background and Theory}{2}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {2.1}Equilibrium propagation}{2}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{\numberline {2.1.1}Implementation in a continuous Hopfield network}{2}\protected@file@percent }
\citation{bengio2015}
\citation{lillicrap2014}
\citation{oconnor2018}
\citation{bullmore2009}
\newlabel{eqn:dynamics}{{4}{3}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {2.1.2}Approximation of the equation of motion}{3}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{\numberline {2.1.3}Other relevant methods for biological plausibility}{3}\protected@file@percent }
\citation{glorot2010}
\citation{watts98}
\citation{bullmore2009}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.2}Vanishing gradient problem}{4}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {2.3}Small-world networks}{4}\protected@file@percent }
\newlabel{sec:sw_background}{{2.3}{4}}
\citation{watts98}
\citation{watts98}
\citation{he2015}
\citation{ioffe2015}
\citation{lee2015}
\citation{xie2003}
\citation{pineda1987}
\citation{lillicrap2014}
\citation{oconnor2018}
\citation{bengio2015}
\citation{???}
\citation{bartunov2018}
\citation{shainline2019}
\citation{davies2018}
\citation{nahmias2013}
\citation{he2015}
\citation{srivastava2015}
\citation{xiaohu2011}
\citation{krishnan2019}
\citation{ioffe2015}
\citation{glorot2010}
\citation{???}
\newlabel{eqn:charpathlength}{{10}{5}}
\newlabel{eqn:clustcoeff}{{11}{5}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.4}Nonlinearities learning residuals}{5}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {3}Related work}{5}\protected@file@percent }
\citation{scellier17}
\citation{pytorch}
\citation{scellier17}
\citation{mnist1998}
\citation{scellier17}
\citation{glorot10}
\providecommand*\caption@xref[2]{\@setref\relax\@undefined{#1}}
\newlabel{fig:top_basic}{{1a}{6}}
\newlabel{sub@fig:top_basic}{{a}{6}}
\newlabel{fig:top_sw}{{1b}{6}}
\newlabel{sub@fig:top_sw}{{b}{6}}
\@writefile{lof}{\contentsline {figure}{\numberline {1}{\ignorespaces \relax }}{6}\protected@file@percent }
\newlabel{fig:topology_illus}{{1}{6}}
\@writefile{toc}{\contentsline {section}{\numberline {4}Implementation}{6}\protected@file@percent }
\newlabel{eqn:hardened_sigmoid}{{12}{6}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.1}Basic topology with unique learning rates}{6}\protected@file@percent }
\newlabel{sec:basic_topology}{{4.1}{6}}
\citation{scellier17}
\newlabel{eqn:gb_init}{{14}{7}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.2}Basic topology with single learning rate}{7}\protected@file@percent }
\newlabel{sec:basic_topology_uniform}{{4.2}{7}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.3}Our topology}{7}\protected@file@percent }
\newlabel{sec:our_topology}{{4.3}{7}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.4}Deep network on a linear dataset}{7}\protected@file@percent }
\newlabel{sec:linear_implementation}{{4.4}{7}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.5}Tracking the training rates of individual pairs of layers}{7}\protected@file@percent }
\newlabel{eqn:rms_correction}{{15}{7}}
\citation{scellier17}
\citation{scellier17}
\citation{mnist1998}
\@writefile{lot}{\contentsline {table}{\numberline {1}{\ignorespaces Hyperparameters of networks tested on MNIST dataset\relax }}{8}\protected@file@percent }
\newlabel{table:hyperparameters}{{1}{8}}
\newlabel{eqn:running_avg}{{16}{8}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.6}Tracking the spread of training rates as a scalar}{8}\protected@file@percent }
\newlabel{eqn:training_sum}{{17}{8}}
\newlabel{eqn:spread}{{18}{8}}
\@writefile{toc}{\contentsline {section}{\numberline {5}Results}{8}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {5.1}MNIST dataset}{8}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{\numberline {5.1.1}Network performance comparison}{8}\protected@file@percent }
\newlabel{sec:network_performance}{{5.1.1}{8}}
\citation{scellier17}
\citation{scellier17}
\@writefile{lof}{\contentsline {figure}{\numberline {2}{\ignorespaces Comparison of network topologies on MNIST dataset. Dotted lines show test error and solid lines show training error. In red is a network with a standard multilayer feedforward topology and a single learning rate. In blue is a network with the same multilayer feedforward topology and unique learning rates for each layer, tuned to promote uniformity in the extent to which each pair of layers trains, as described in \cite  {scellier17}. In green is a network with a multilayer feedforward topology with fully connected layers and with around $8\%$ of its connections replaced by random layer-skipping connections.\relax }}{9}\protected@file@percent }
\newlabel{fig:mnist_comparison}{{2}{9}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {5.1.2}Training rates of individual pairs of layers}{9}\protected@file@percent }
\newlabel{sec:mnist_perlayer}{{5.1.2}{9}}
\@writefile{lof}{\contentsline {figure}{\numberline {3}{\ignorespaces Observation of extent of training of individual layers for different network topologies. Measurements were taken while running trials shown in figure 2\hbox {}, and have been averaged as described in (\G@refundefinedtrue \text  {\normalfont  \bfseries  ??}\GenericWarning  {               }{LaTeX Warning: Reference `eqn:layer_averaging' on page 10 undefined}). To the left is a network with a standard multilayer feedforward topology and a single learning rate. In the center is a network with the same multilayer feedforward topology and unique learning rates for each layer, as described in \cite  {scellier17}. To the right is a network with a multilayer feedforward topology with fully-connected layers and with around $8\%$ of its connections replaced by random layer-skipping connections.\relax }}{10}\protected@file@percent }
\newlabel{fig:mnist_layers}{{3}{10}}
\@writefile{lof}{\contentsline {figure}{\numberline {4}{\ignorespaces The training error after one epoch of a network with our topology, as connections are replaced.\relax }}{10}\protected@file@percent }
\newlabel{fig:mnist_1epoch}{{4}{10}}
\@writefile{lof}{\contentsline {figure}{\numberline {5}{\ignorespaces Training rates of pairs of layers in a 15-layer network trained on a linear dataset. To the left is a network with no replaced connections. To the right is a network with 10\% of its connections replaced. Trace color changes from yellow for shallow layer pairs to purple for deep layer pairs.\relax }}{11}\protected@file@percent }
\newlabel{fig:linear_layers}{{5}{11}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {5.1.3}Error rate after one epoch as connections are added}{11}\protected@file@percent }
\newlabel{sec:mnist_1epoch}{{5.1.3}{11}}
\@writefile{toc}{\contentsline {subsection}{\numberline {5.2}Linear dataset}{11}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{\numberline {5.2.1}Training rates of individual pairs of layers}{11}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {6}{\ignorespaces Spread, cost function after one epoch, and small-world metrics of a 15-layer network trained on a linear dataset with a varying proportion of its connections replaced. Green: spread of training rates of layer pairs. Red: cost function after training for 1 epoch. Blue: characteristic path length. Red: clustering coefficient.\relax }}{12}\protected@file@percent }
\newlabel{fig:linear_spread}{{6}{12}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {5.2.2}Spread, cost after one epoch, and small-world metrics}{12}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {6}Conclusion}{12}\protected@file@percent }
\bibstyle{plain}
\citation{*}
\bibdata{references}
\bibcite{gammell19}{1}
\bibcite{bartunov2018}{2}
\bibcite{bengio2015}{3}
\bibcite{bullmore2009}{4}
\bibcite{davies2018}{5}
\bibcite{glorot2010}{6}
\bibcite{he2015}{7}
\bibcite{hopfield1984}{8}
\bibcite{ioffe2015}{9}
\bibcite{krishnan2019}{10}
\bibcite{mnist1998}{11}
\bibcite{lee2015}{12}
\bibcite{lillicrap2014}{13}
\bibcite{nahmias2013}{14}
\bibcite{oconnor2018}{15}
\bibcite{pineda1987}{16}
\bibcite{scellier17}{17}
\bibcite{shainline2019}{18}
\bibcite{srivastava2015}{19}
\bibcite{watts98}{20}
\bibcite{xiaohu2011}{21}
\bibcite{xie2003}{22}
