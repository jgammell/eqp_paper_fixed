\relax 
\citation{scellier17}
\citation{hopfield1984}
\citation{bengio2015}
\citation{scellier17}
\citation{scellier17}
\citation{mnist1998}
\citation{simonyan2014,srivastava2015tvdn}
\citation{scellier17}
\citation{watts98}
\citation{bartunov2018}
\citation{scellier17}
\citation{bullmore2009}
\citation{he2015,srivastava2015}
\citation{xiaohu2011,krishnan2019}
\providecommand \oddpage@label [2]{}
\@writefile{toc}{\contentsline {section}{\numberline {1}Introduction}{1}\protected@file@percent }
\citation{scellier17}
\citation{hopfield1984}
\citation{bengio2015}
\citation{scellier17}
\@writefile{toc}{\contentsline {section}{\numberline {2}Background and Theory}{2}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {2.1}Equilibrium propagation}{2}\protected@file@percent }
\newlabel{sec:eqp_formulation}{{2.1}{2}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {2.1.1}Implementation in a continuous Hopfield network}{2}\protected@file@percent }
\newlabel{eqn:hardened_sigmoid}{{1}{2}}
\newlabel{eqn:cost}{{4}{3}}
\newlabel{eqn:dynamics}{{6}{3}}
\newlabel{eqn:weight_correction}{{7}{3}}
\citation{lillicrap2014}
\citation{oconnor2018}
\citation{bullmore2009}
\citation{scellier17}
\citation{glorot2010}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {2.1.2}Approximation of equation of motion}{4}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{\numberline {2.1.3}Applicable methods for enhanced biological-plausibility}{4}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {2.2}Vanishing gradient problem}{4}\protected@file@percent }
\newlabel{sec:vangrad}{{2.2}{4}}
\citation{watts98}
\citation{bullmore2009}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {2.2.1}Contribution by approximation of equation of motion}{5}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {2.3}Small-world networks}{5}\protected@file@percent }
\newlabel{sec:sw_background}{{2.3}{5}}
\citation{watts98}
\citation{watts98}
\citation{he2015}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {2.3.1}Small-world metrics}{6}\protected@file@percent }
\newlabel{eqn:charpathlength}{{15}{6}}
\newlabel{eqn:clustcoeff}{{16}{6}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {2.3.2}Small-world metrics have little correlation with network performance}{6}\protected@file@percent }
\citation{he2015,ioffe2015}
\citation{lee2015}
\citation{xie2003}
\citation{pineda1987}
\citation{lillicrap2014}
\citation{oconnor2018}
\citation{bengio2015}
\citation{???}
\citation{bartunov2018}
\citation{shainline2019}
\citation{davies2018}
\citation{nahmias2013}
\citation{he2015}
\citation{srivastava2015}
\citation{xiaohu2011}
\citation{krishnan2019}
\citation{ioffe2015}
\citation{glorot2010}
\citation{???}
\citation{scellier17}
\citation{scellier17}
\citation{scellier17}
\citation{pytorch}
\citation{scellier17}
\citation{mnist1998}
\citation{scellier17}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.4}Nonlinearities learning residuals}{7}\protected@file@percent }
\newlabel{sec:residuals}{{2.4}{7}}
\@writefile{toc}{\contentsline {section}{\numberline {3}Related work}{7}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {4}Implementation}{7}\protected@file@percent }
\citation{scellier17}
\citation{glorot2010}
\citation{scellier17}
\providecommand*\caption@xref[2]{\@setref\relax\@undefined{#1}}
\newlabel{fig:top_basic}{{1a}{8}}
\newlabel{sub@fig:top_basic}{{a}{8}}
\newlabel{fig:top_sw}{{1b}{8}}
\newlabel{sub@fig:top_sw}{{b}{8}}
\@writefile{lof}{\contentsline {figure}{\numberline {1}{\ignorespaces \relax }}{8}\protected@file@percent }
\newlabel{fig:topology_illus}{{1}{8}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.1}Basic topology with unique learning rates}{8}\protected@file@percent }
\newlabel{sec:basic_topology}{{4.1}{8}}
\newlabel{eqn:gb_init}{{17}{8}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.2}Basic topology with single learning rate}{8}\protected@file@percent }
\newlabel{sec:basic_topology_uniform}{{4.2}{8}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.3}Our topology}{8}\protected@file@percent }
\newlabel{sec:our_topology}{{4.3}{8}}
\citation{scellier17}
\citation{scellier17}
\citation{mnist1998}
\@writefile{loa}{\contentsline {algocf}{\numberline {1}{\ignorespaces The procedure by which we modify a network's topology.\relax }}{9}\protected@file@percent }
\newlabel{alg:gentop}{{1}{9}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.4}Tracking the training rates of individual pairs of layers}{9}\protected@file@percent }
\newlabel{eqn:rms_correction}{{18}{9}}
\newlabel{eqn:running_avg}{{19}{9}}
\@writefile{toc}{\contentsline {section}{\numberline {5}Results}{9}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {2}{\ignorespaces Comparison of performance of network topologies on MNIST dataset. Dotted lines show test error and solid lines show training error. In red is a network with the basic topology and a single learning rate (section 4.2\hbox {}). In blue is a network with the basic topology and unique learning rates (section 4.1\hbox {}), tuned to counter the vanishing gradient problem; this is a recreation of the 5-layer network in \cite  {scellier17}. In green is a network with our topology, $p=7.56\%$ (section 4.3\hbox {}). The network with a basic topology and a single learning rate performs poorly because it suffers from the vanishing gradient problem. The problem can be solved by introducing unique learning rates, or by implementing our topology.\relax }}{10}\protected@file@percent }
\newlabel{fig:mnist_comparison}{{2}{10}}
\@writefile{lot}{\contentsline {table}{\numberline {1}{\ignorespaces Hyperparameters of networks tested on MNIST dataset\relax }}{10}\protected@file@percent }
\newlabel{table:hyperparameters}{{1}{10}}
\citation{scellier17}
\citation{scellier17}
\@writefile{lof}{\contentsline {figure}{\numberline {3}{\ignorespaces Observation of extent of training of individual layers for different network topologies. Measurements were taken while running trials shown in figure 2\hbox {}, and have been averaged as described in (\G@refundefinedtrue \text  {\normalfont  \bfseries  ??}\GenericWarning  {               }{LaTeX Warning: Reference `eqn:layer_averaging' on page 11 undefined}). To the left is a network with a standard multilayer feedforward topology and a single learning rate. In the center is a network with the same multilayer feedforward topology and unique learning rates for each layer, as described in \cite  {scellier17}. To the right is a network with a multilayer feedforward topology with fully-connected layers and $p=7.56\%$.\relax }}{11}\protected@file@percent }
\newlabel{fig:mnist_layers}{{3}{11}}
\@writefile{toc}{\contentsline {subsection}{\numberline {5.1}Network performance comparison}{11}\protected@file@percent }
\newlabel{sec:network_performance}{{5.1}{11}}
\@writefile{toc}{\contentsline {subsection}{\numberline {5.2}Training rates of individual pairs of layers}{11}\protected@file@percent }
\newlabel{sec:mnist_perlayer}{{5.2}{11}}
\@writefile{lof}{\contentsline {figure}{\numberline {4}{\ignorespaces Performance of a network with our topology (section 4.3\hbox {}) with varying $p$. The top graph shows the training error after one epoch. The bottom graph shows the extent to which weights connecting each pair of layers was corrected over the epoch. It can be seen that there is little improvement for $p<10^{-4}$, rapid improvement for $10^{-4}<p<10^{-2}$ and little improvement for $p>10^{-2}$. The training error after one epoch decreases as pairs of layers train more uniformly with respect to depth.\relax }}{12}\protected@file@percent }
\newlabel{fig:mnist_1epoch}{{4}{12}}
\@writefile{toc}{\contentsline {subsection}{\numberline {5.3}Error rate after one epoch as connections are added}{12}\protected@file@percent }
\newlabel{sec:mnist_1epoch}{{5.3}{12}}
\@writefile{toc}{\contentsline {section}{\numberline {6}Conclusion}{12}\protected@file@percent }
\bibstyle{plain}
\citation{*}
\bibdata{references}
\bibcite{gammell19}{1}
\bibcite{bartunov2018}{2}
\bibcite{bengio2015}{3}
\bibcite{bullmore2009}{4}
\bibcite{davies2018}{5}
\bibcite{glorot2010}{6}
\bibcite{he2015}{7}
\bibcite{hopfield1984}{8}
\bibcite{ioffe2015}{9}
\bibcite{krishnan2019}{10}
\bibcite{mnist1998}{11}
\bibcite{lee2015}{12}
\bibcite{lillicrap2014}{13}
\bibcite{nahmias2013}{14}
\bibcite{oconnor2018}{15}
\bibcite{pineda1987}{16}
\bibcite{scellier17}{17}
\bibcite{shainline2019}{18}
\bibcite{simonyan2014}{19}
\bibcite{srivastava2015tvdn}{20}
\bibcite{srivastava2015}{21}
\bibcite{watts98}{22}
\bibcite{xiaohu2011}{23}
\bibcite{xie2003}{24}
